{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'eval_psnr_ssim' from 'test' (d:\\Miniconda3\\envs\\pytorch\\lib\\test\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ARCOBA~1\\AppData\\Local\\Temp/ipykernel_22076/132683605.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_coord\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mset_save_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mssim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meval_psnr_ssim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'eval_psnr_ssim' from 'test' (d:\\Miniconda3\\envs\\pytorch\\lib\\test\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import yaml\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.optim import Adam\n",
    "from torchvision.utils import save_image\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "\n",
    "sys.path.append(\"models\")\n",
    "import models\n",
    "from models import losses\n",
    "from models.liif import LIIF\n",
    "from models.discriminator import Discriminator\n",
    "from models.losses import AdversarialLoss\n",
    "\n",
    "import utils\n",
    "from utils import make_coord,set_save_path,ssim\n",
    "import datasets\n",
    "from test import eval_psnr_ssim, batched_predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(r'load\\div2k\\DIV2K_valid_LR_bicubic\\X2\\0802x2.png')\n",
    "#img = transforms.Resize((int(img.height/2),int(img.width/2)),Image.BICUBIC)(img)\n",
    "timg = transforms.ToTensor()(img) #[3,LR_H,LR_W]\n",
    "edgemap = edge.laplacian_kernel(timg.unsqueeze(0).cuda())\n",
    "transforms.ToPILImage()(edgemap[0]).save('0802x2edge.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: d:\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "#pytorch æ±‚LPIPS\n",
    " \n",
    "import torch\n",
    "import lpips\n",
    "import os\n",
    " \n",
    "use_gpu = False         # Whether to use GPU\n",
    "spatial = False         # Return a spatial map of perceptual distance.\n",
    " \n",
    "# Linearly calibrated models (LPIPS)\n",
    "loss_fn = lpips.LPIPS(net='vgg', spatial=spatial) # Can also set net = 'squeeze' or 'vgg'\n",
    "# loss_fn = lpips.LPIPS(net='alex', spatial=spatial, lpips=False) # Can also set net = 'squeeze' or 'vgg'\n",
    " \n",
    "if(use_gpu):\n",
    "\tloss_fn.cuda()\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage with dummy tensors\n",
    "rood_path = r'D:\\Project\\results\\faces'\n",
    "hr_path = r'E:\\Code\\Python\\datas\\selfWHURS\\WHURS19-test\\GT'\n",
    "sr_path = r'E:\\Code\\Python\\liif-self\\result\\WHURS19_edsrblx4'\n",
    "\n",
    "hr_path_list = []\n",
    "sr_path_list = []\n",
    "## path in net is already exist\n",
    "\n",
    "for root, _, fnames in sorted(os.walk(hr_path, followlinks=True)):\n",
    "\tfor fname in fnames:\n",
    "\t\tpath = os.path.join(hr_path, fname)\n",
    "\t\thr_path_list.append(path)\n",
    "\n",
    "for root, _, fnames in sorted(os.walk(sr_path, followlinks=True)):\n",
    "\tfor fname in fnames:\n",
    "\t\tpath = os.path.join(sr_path, fname)\n",
    "\t\tsr_path_list.append(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avarage Distances: 0.304\n"
     ]
    }
   ],
   "source": [
    "dist_ = []\n",
    "for i in range(len(hr_path_list)):\n",
    "\thr_img = lpips.im2tensor(lpips.load_image(hr_path_list[i]))\n",
    "\tsr_img = lpips.im2tensor(lpips.load_image(sr_path_list[i]))\n",
    "\tif(use_gpu):\n",
    "\t\thr_img = hr_img.cuda()\n",
    "\t\tsr_img = sr_img.cuda()\n",
    "\tdist = loss_fn.forward(hr_img, sr_img)\n",
    "\tdist_.append(dist.mean().item())\n",
    "print('Avarage Distances: %.3f' % (sum(dist_)/len(hr_path_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2441, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_img = lpips.im2tensor(lpips.load_image(r'E:\\Code\\Python\\liif-self\\result\\WHURS19_edsrblx4\\airport_41.png'))\n",
    "sr_img = lpips.im2tensor(lpips.load_image(r'E:\\Code\\Python\\datas\\selfWHURS\\WHURS19-test\\GT\\airport_41.jpg'))\n",
    "if(use_gpu):\n",
    "    hr_img = hr_img.cuda()\n",
    "    sr_img = sr_img.cuda()\n",
    "dist = loss_fn.forward(hr_img, sr_img)\n",
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2435, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_img = lpips.im2tensor(lpips.load_image(r'E:\\Code\\Python\\liif-self\\result\\WHURS19_samx_L0Sgradx4\\airport_41.png'))\n",
    "sr_img = lpips.im2tensor(lpips.load_image(r'E:\\Code\\Python\\datas\\selfWHURS\\WHURS19-test\\GT\\airport_41.jpg'))\n",
    "if(use_gpu):\n",
    "    hr_img = hr_img.cuda()\n",
    "    sr_img = sr_img.cuda()\n",
    "dist = loss_fn.forward(hr_img, sr_img)\n",
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelpath = r'weights\\edsr-baseline-liif.pth'\n",
    "lr_path = r'testimg\\div2klrx4\\0801x4.png'\n",
    "hr_path = r'load\\div2k\\DIV2K_valid_HR\\0801.png'\n",
    "sr_path = r'testimg\\ouput.jpg'\n",
    "\n",
    "lr = transforms.ToTensor()(Image.open(lr_path))\n",
    "hr = transforms.ToTensor()(Image.open(hr_path))\n",
    "model = models.make(torch.load(modelpath)['model'], load_sd=True).cuda()\n",
    "lr = ((lr - 0.5) / 0.5).cuda().unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    feat = model.gen_feat(lr)\n",
    "featimg= transforms.ToPILImage()(feat[0][0])\n",
    "plt.imshow(featimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res2neck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, baseWidth=26, scale = 4):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            inplanes: input channel dimensionality\n",
    "            planes: output channel dimensionality\n",
    "            baseWidth: basic width of conv3x3\n",
    "            scale: number of scale.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        width = int(math.floor(planes * (baseWidth/64.0)))\n",
    "        self.conv1 = nn.Conv2d(inplanes, width*scale, kernel_size=1, bias=False)\n",
    "\n",
    "        self.nums = scale-1 if scale !=1 else 1\n",
    "        convs = []\n",
    "        for i in range(self.nums):\n",
    "          convs.append(nn.Conv2d(width, width, kernel_size=3, stride = 1, padding=1, bias=False))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(width*scale, planes, kernel_size=1, bias=False)\n",
    "        self.relu = nn.LeakyReLU(0.01, inplace=True)\n",
    "\n",
    "        self.scale = scale\n",
    "        self.width = width\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "          print(\"i spx\",i,spx[i].shape)\n",
    "          sp = spx[i] if i==0 else sp + spx[i]\n",
    "          sp = self.relu(self.convs[i](sp))\n",
    "          out = sp if i==0 else torch.cat((out, sp), 1)\n",
    "\n",
    "        if self.scale != 1 :\n",
    "          out = torch.cat((out, spx[self.nums]),1)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        print(\"out \",out.shape)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "neck = Res2neck(64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Res2neck(\n",
       "  (conv1): Conv2d(64, 104, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(26, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): Conv2d(26, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): Conv2d(26, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (conv3): Conv2d(104, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i spx 0 torch.Size([1, 26, 224, 224])\n",
      "i spx 1 torch.Size([1, 26, 224, 224])\n",
      "i spx 2 torch.Size([1, 26, 224, 224])\n",
      "out  torch.Size([1, 64, 224, 224])\n",
      "torch.Size([1, 64, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "inf = torch.rand(1,64,224,224)\n",
    "outf = neck(inf)\n",
    "print(outf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Get_lap_gradient(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        kernel = [[-1, -1, -1],\n",
    "                    [-1, 8, -1],\n",
    "                    [-1, -1, -1]]\n",
    "        kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
    "        self.weight = nn.Parameter(data=kernel, requires_grad=False).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x[:, 0]\n",
    "        x1 = x[:, 1]\n",
    "        x2 = x[:, 2]\n",
    "        x0 = F.conv2d(x0.unsqueeze(1), self.weight, padding=1)\n",
    "        x1 = F.conv2d(x1.unsqueeze(1), self.weight, padding=1)\n",
    "        x2 = F.conv2d(x2.unsqueeze(1), self.weight, padding=1)\n",
    "        x = torch.cat([x0, x1, x2], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lap = Get_lap_gradient()\n",
    "out = lap(inf.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "5b5292cc5f82d8561d05c044f8ed41e8f78352f5ad17a4513f2686858f295381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
