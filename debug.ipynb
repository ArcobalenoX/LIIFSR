{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.optim import Adam\n",
    "from torchvision.utils import save_image\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "\n",
    "sys.path.append(\"models\")\n",
    "import models\n",
    "from models import losses\n",
    "from models.liif import LIIF\n",
    "from models.discriminator import Discriminator\n",
    "from models.losses import AdversarialLoss\n",
    "\n",
    "import utils\n",
    "from utils import make_coord,set_save_path,ssim\n",
    "import datasets\n",
    "from test import eval_psnr_ssim, batched_predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(r'load\\div2k\\DIV2K_valid_LR_bicubic\\X2\\0802x2.png')\n",
    "#img = transforms.Resize((int(img.height/2),int(img.width/2)),Image.BICUBIC)(img)\n",
    "timg = transforms.ToTensor()(img) #[3,LR_H,LR_W]\n",
    "edgemap = edge.laplacian_kernel(timg.unsqueeze(0).cuda())\n",
    "transforms.ToPILImage()(edgemap[0]).save('0802x2edge.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: d:\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "#pytorch æ±‚LPIPS\n",
    " \n",
    "import torch\n",
    "import lpips\n",
    "import os\n",
    " \n",
    "use_gpu = False         # Whether to use GPU\n",
    "spatial = False         # Return a spatial map of perceptual distance.\n",
    " \n",
    "# Linearly calibrated models (LPIPS)\n",
    "loss_fn = lpips.LPIPS(net='vgg', spatial=spatial) # Can also set net = 'squeeze' or 'vgg'\n",
    "# loss_fn = lpips.LPIPS(net='alex', spatial=spatial, lpips=False) # Can also set net = 'squeeze' or 'vgg'\n",
    " \n",
    "if(use_gpu):\n",
    "\tloss_fn.cuda()\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage with dummy tensors\n",
    "rood_path = r'D:\\Project\\results\\faces'\n",
    "hr_path = r'E:\\Code\\Python\\datas\\selfWHURS\\WHURS19-test\\GT'\n",
    "sr_path = r'E:\\Code\\Python\\liif-self\\result\\WHURS19_edsrblx4'\n",
    "\n",
    "hr_path_list = []\n",
    "sr_path_list = []\n",
    "## path in net is already exist\n",
    "\n",
    "for root, _, fnames in sorted(os.walk(hr_path, followlinks=True)):\n",
    "\tfor fname in fnames:\n",
    "\t\tpath = os.path.join(hr_path, fname)\n",
    "\t\thr_path_list.append(path)\n",
    "\n",
    "for root, _, fnames in sorted(os.walk(sr_path, followlinks=True)):\n",
    "\tfor fname in fnames:\n",
    "\t\tpath = os.path.join(sr_path, fname)\n",
    "\t\tsr_path_list.append(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avarage Distances: 0.304\n"
     ]
    }
   ],
   "source": [
    "dist_ = []\n",
    "for i in range(len(hr_path_list)):\n",
    "\thr_img = lpips.im2tensor(lpips.load_image(hr_path_list[i]))\n",
    "\tsr_img = lpips.im2tensor(lpips.load_image(sr_path_list[i]))\n",
    "\tif(use_gpu):\n",
    "\t\thr_img = hr_img.cuda()\n",
    "\t\tsr_img = sr_img.cuda()\n",
    "\tdist = loss_fn.forward(hr_img, sr_img)\n",
    "\tdist_.append(dist.mean().item())\n",
    "print('Avarage Distances: %.3f' % (sum(dist_)/len(hr_path_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2441, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_img = lpips.im2tensor(lpips.load_image(r'E:\\Code\\Python\\liif-self\\result\\WHURS19_edsrblx4\\airport_41.png'))\n",
    "sr_img = lpips.im2tensor(lpips.load_image(r'E:\\Code\\Python\\datas\\selfWHURS\\WHURS19-test\\GT\\airport_41.jpg'))\n",
    "if(use_gpu):\n",
    "    hr_img = hr_img.cuda()\n",
    "    sr_img = sr_img.cuda()\n",
    "dist = loss_fn.forward(hr_img, sr_img)\n",
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2435, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_img = lpips.im2tensor(lpips.load_image(r'E:\\Code\\Python\\liif-self\\result\\WHURS19_samx_L0Sgradx4\\airport_41.png'))\n",
    "sr_img = lpips.im2tensor(lpips.load_image(r'E:\\Code\\Python\\datas\\selfWHURS\\WHURS19-test\\GT\\airport_41.jpg'))\n",
    "if(use_gpu):\n",
    "    hr_img = hr_img.cuda()\n",
    "    sr_img = sr_img.cuda()\n",
    "dist = loss_fn.forward(hr_img, sr_img)\n",
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelpath = r'weights\\edsr-baseline-liif.pth'\n",
    "lr_path = r'testimg\\div2klrx4\\0801x4.png'\n",
    "hr_path = r'load\\div2k\\DIV2K_valid_HR\\0801.png'\n",
    "sr_path = r'testimg\\ouput.jpg'\n",
    "\n",
    "lr = transforms.ToTensor()(Image.open(lr_path))\n",
    "hr = transforms.ToTensor()(Image.open(hr_path))\n",
    "model = models.make(torch.load(modelpath)['model'], load_sd=True).cuda()\n",
    "lr = ((lr - 0.5) / 0.5).cuda().unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    feat = model.gen_feat(lr)\n",
    "featimg= transforms.ToPILImage()(feat[0][0])\n",
    "plt.imshow(featimg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "5b5292cc5f82d8561d05c044f8ed41e8f78352f5ad17a4513f2686858f295381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
